{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"notes/api-design/RESTful%20API%20design/","text":"RESTful web API design RESTful web API design Representational State Transfer (REST) as an architectural approach to designing web services. An API should be platform independent, meaning any client can call the API regardless of how it is implemented internally. It should also be backwards compatible. Additional functionality can be added without breaking the existing ones. REST design principles REST APIs are designed around resources. Each resource has a unique identified, a URI identified the resource, e.g. https://adventure-works.com/orders/1 . Client interact with it using representations of resources, e.g. json . For REST APIs built on HTTP, the uniform interface includes using standard HTTP verbs to perform operations on resources. The most common operations are GET, POST, PUT, PATCH, and DELETE. REST APIs use a stateless request model. Organise API design around resources. When possible, resource URIs should be based on nouns (the resource) and not verbs (the operations on the resource). Avoid creating APIs that simply mirror the internal structure of a database. The purpose of REST is to model entities and the operations that an application can perform on those entities. A client should not be exposed to the internal implementation. Adopt a consistent naming convention in URIs. In more complex systems, it can be tempting to provide URIs that enable a client to navigate through several levels of relationships, such as /customers/1/orders/99/products . However, this level of complexity can be difficult to maintain and is inflexible if the relationships between resources change in the future. Instead, try to keep URIs relatively simple. Once an application has a reference to a resource, it should be possible to use this reference to find items related to that resource. The preceding query can be replaced with the URI /customers/1/orders to find all the orders for customer 1, and then /orders/99/products to find the products in this order. Avoid sending too many requests, instead denormalize the data and combine related information into bigger resources that can be retrieved with a single request. The differences between POST, PUT, and PATCH A POST request creates new a resource. A PUT create a new or updates an existing one. A PATCH request performs a partial update to an existing resource. The client specifies the URI for the resource. The request body specifies a set of changes to apply to the resource. This can be more efficient than using PUT, because the client only sends the changes, not the entire representation of the resource. Asynchronous operations Status code 202 (Accepted) should returned together with the Location header, so the client can poll the status endpoint for status. HTTP / 1.1 202 Accepted Location : /api/status/12345 If the client sends a GET request to the status endpoint, it should return the current progress potentially with URI to cancel the operation. HTTP / 1.1 200 OK Content-Type: application/json { \"status\":\"In progress\", \"link\": { \"rel\":\"cancel\", \"method\":\"delete\", \"href\":\"/api/status/12345\" } } If an async operation creates a resource that might take a long time, the status endpoint should return code 303 (See Other) after the operation completes. The header should contain *Location that gives the URI of the new resource. HTTP / 1.1 303 See Other Location : /api/orders/12345 Filter and pagination Allow passing filter in the query string of the URI such as /order?minCost=n and return filtered result. Also considering imposing upper limit on the number of items returned, to help prevent Denial of Service attack. Query strings can also be used for sorting, however this might not be idea for caching as many caching methods use URI as the key. Use HATEOAS to enable navigation to related resources This principle is known as HATEOAS, or Hypertext as the Engine of Application State. Each HTTP GET request should return the information necessary to find the resources related directly to the requested object through hyperlinks included in the response. It should also be provided with information that describes the operations available on each of these resources. For example, to handle the relationship between an order and a customer, the representation of an order could include links that identify the available operations for the customer of the order. Here is a possible representation: { \"orderID\" : 3 , \"productID\" : 2 , \"quantity\" : 4 , \"orderValue\" : 16.60 , \"links\" :[ { \"rel\" : \"customer\" , \"href\" : \"https://adventure-works.com/customers/3\" , \"action\" : \"GET\" , \"types\" :[ \"text/xml\" , \"application/json\" ] }, { \"rel\" : \"customer\" , \"href\" : \"https://adventure-works.com/customers/3\" , \"action\" : \"PUT\" , \"types\" :[ \"application/x-www-form-urlencoded\" ] }, { \"rel\" : \"customer\" , \"href\" : \"https://adventure-works.com/customers/3\" , \"action\" : \"DELETE\" , \"types\" :[ ] }, { \"rel\" : \"self\" , \"href\" : \"https://adventure-works.com/orders/3\" , \"action\" : \"GET\" , \"types\" :[ \"text/xml\" , \"application/json\" ] }, { \"rel\" : \"self\" , \"href\" : \"https://adventure-works.com/orders/3\" , \"action\" : \"PUT\" , \"types\" :[ \"application/x-www-form-urlencoded\" ] }, { \"rel\" : \"self\" , \"href\" : \"https://adventure-works.com/orders/3\" , \"action\" : \"DELETE\" , \"types\" :[ ] } ] } Versioning a RESTful web API No versioning Adding content to existing resources might not present a breaking change as client applications that are not expecting to see this content will ignore it. However, if more radical changes to the schema of resources occur (such as removing or renaming fields) or the relationships between resources change then these may constitute breaking changes that prevent existing client applications from functioning correctly. URI versioning The version of the resource could be exposed through a URI containing a version number, such as https://adventure-works.com/v2/customers/3 . This versioning mechanism is very simple but depends on the server routing the request to the appropriate endpoint. However, it can become unwieldy as the web API matures through several iterations and the server has to support a number of different versions. HATEOAS can be complicated as all links need to reference the new version. Query string versioning Include version as query string of URL, https://adventure-works.com/customers/3?version=2 . This approach also suffers from the same complications for implementing HATEOAS as the URI versioning mechanism. Header versioning Rather than appending the version number as a query string parameter, you could implement a custom header that indicates the version of the resource. This approach requires that the client application adds the appropriate header to any requests, although the code handling the client request could use a default value (version 1) if the version header is omitted. Microsoft REST API Design Guideline should be used as a reference.","title":"RESTful web API design"},{"location":"notes/api-design/RESTful%20API%20design/#restful-web-api-design","text":"RESTful web API design Representational State Transfer (REST) as an architectural approach to designing web services. An API should be platform independent, meaning any client can call the API regardless of how it is implemented internally. It should also be backwards compatible. Additional functionality can be added without breaking the existing ones.","title":"RESTful web API design"},{"location":"notes/api-design/RESTful%20API%20design/#rest-design-principles","text":"REST APIs are designed around resources. Each resource has a unique identified, a URI identified the resource, e.g. https://adventure-works.com/orders/1 . Client interact with it using representations of resources, e.g. json . For REST APIs built on HTTP, the uniform interface includes using standard HTTP verbs to perform operations on resources. The most common operations are GET, POST, PUT, PATCH, and DELETE. REST APIs use a stateless request model. Organise API design around resources. When possible, resource URIs should be based on nouns (the resource) and not verbs (the operations on the resource). Avoid creating APIs that simply mirror the internal structure of a database. The purpose of REST is to model entities and the operations that an application can perform on those entities. A client should not be exposed to the internal implementation. Adopt a consistent naming convention in URIs. In more complex systems, it can be tempting to provide URIs that enable a client to navigate through several levels of relationships, such as /customers/1/orders/99/products . However, this level of complexity can be difficult to maintain and is inflexible if the relationships between resources change in the future. Instead, try to keep URIs relatively simple. Once an application has a reference to a resource, it should be possible to use this reference to find items related to that resource. The preceding query can be replaced with the URI /customers/1/orders to find all the orders for customer 1, and then /orders/99/products to find the products in this order. Avoid sending too many requests, instead denormalize the data and combine related information into bigger resources that can be retrieved with a single request.","title":"REST design principles"},{"location":"notes/api-design/RESTful%20API%20design/#the-differences-between-post-put-and-patch","text":"A POST request creates new a resource. A PUT create a new or updates an existing one. A PATCH request performs a partial update to an existing resource. The client specifies the URI for the resource. The request body specifies a set of changes to apply to the resource. This can be more efficient than using PUT, because the client only sends the changes, not the entire representation of the resource.","title":"The differences between POST, PUT, and PATCH"},{"location":"notes/api-design/RESTful%20API%20design/#asynchronous-operations","text":"Status code 202 (Accepted) should returned together with the Location header, so the client can poll the status endpoint for status. HTTP / 1.1 202 Accepted Location : /api/status/12345 If the client sends a GET request to the status endpoint, it should return the current progress potentially with URI to cancel the operation. HTTP / 1.1 200 OK Content-Type: application/json { \"status\":\"In progress\", \"link\": { \"rel\":\"cancel\", \"method\":\"delete\", \"href\":\"/api/status/12345\" } } If an async operation creates a resource that might take a long time, the status endpoint should return code 303 (See Other) after the operation completes. The header should contain *Location that gives the URI of the new resource. HTTP / 1.1 303 See Other Location : /api/orders/12345","title":"Asynchronous operations"},{"location":"notes/api-design/RESTful%20API%20design/#filter-and-pagination","text":"Allow passing filter in the query string of the URI such as /order?minCost=n and return filtered result. Also considering imposing upper limit on the number of items returned, to help prevent Denial of Service attack. Query strings can also be used for sorting, however this might not be idea for caching as many caching methods use URI as the key.","title":"Filter and pagination"},{"location":"notes/api-design/RESTful%20API%20design/#use-hateoas-to-enable-navigation-to-related-resources","text":"This principle is known as HATEOAS, or Hypertext as the Engine of Application State. Each HTTP GET request should return the information necessary to find the resources related directly to the requested object through hyperlinks included in the response. It should also be provided with information that describes the operations available on each of these resources. For example, to handle the relationship between an order and a customer, the representation of an order could include links that identify the available operations for the customer of the order. Here is a possible representation: { \"orderID\" : 3 , \"productID\" : 2 , \"quantity\" : 4 , \"orderValue\" : 16.60 , \"links\" :[ { \"rel\" : \"customer\" , \"href\" : \"https://adventure-works.com/customers/3\" , \"action\" : \"GET\" , \"types\" :[ \"text/xml\" , \"application/json\" ] }, { \"rel\" : \"customer\" , \"href\" : \"https://adventure-works.com/customers/3\" , \"action\" : \"PUT\" , \"types\" :[ \"application/x-www-form-urlencoded\" ] }, { \"rel\" : \"customer\" , \"href\" : \"https://adventure-works.com/customers/3\" , \"action\" : \"DELETE\" , \"types\" :[ ] }, { \"rel\" : \"self\" , \"href\" : \"https://adventure-works.com/orders/3\" , \"action\" : \"GET\" , \"types\" :[ \"text/xml\" , \"application/json\" ] }, { \"rel\" : \"self\" , \"href\" : \"https://adventure-works.com/orders/3\" , \"action\" : \"PUT\" , \"types\" :[ \"application/x-www-form-urlencoded\" ] }, { \"rel\" : \"self\" , \"href\" : \"https://adventure-works.com/orders/3\" , \"action\" : \"DELETE\" , \"types\" :[ ] } ] }","title":"Use HATEOAS to enable navigation to related resources"},{"location":"notes/api-design/RESTful%20API%20design/#versioning-a-restful-web-api","text":"","title":"Versioning a RESTful web API"},{"location":"notes/api-design/RESTful%20API%20design/#no-versioning","text":"Adding content to existing resources might not present a breaking change as client applications that are not expecting to see this content will ignore it. However, if more radical changes to the schema of resources occur (such as removing or renaming fields) or the relationships between resources change then these may constitute breaking changes that prevent existing client applications from functioning correctly.","title":"No versioning"},{"location":"notes/api-design/RESTful%20API%20design/#uri-versioning","text":"The version of the resource could be exposed through a URI containing a version number, such as https://adventure-works.com/v2/customers/3 . This versioning mechanism is very simple but depends on the server routing the request to the appropriate endpoint. However, it can become unwieldy as the web API matures through several iterations and the server has to support a number of different versions. HATEOAS can be complicated as all links need to reference the new version.","title":"URI versioning"},{"location":"notes/api-design/RESTful%20API%20design/#query-string-versioning","text":"Include version as query string of URL, https://adventure-works.com/customers/3?version=2 . This approach also suffers from the same complications for implementing HATEOAS as the URI versioning mechanism.","title":"Query string versioning"},{"location":"notes/api-design/RESTful%20API%20design/#header-versioning","text":"Rather than appending the version number as a query string parameter, you could implement a custom header that indicates the version of the resource. This approach requires that the client application adds the appropriate header to any requests, although the code handling the client request could use a default value (version 1) if the version header is omitted. Microsoft REST API Design Guideline should be used as a reference.","title":"Header versioning"},{"location":"notes/aws/AWS%20solution%20architect%20associate/","text":"AWS solution architect associate High Level Services Difference between Regions, Availability Zones and Edge Location Availability zone is basically a data centre, or multiple data centres are close to each other. Region is a geographical area, contains two or more availability zones. Each region is a separate geographic area. * Each region has multiple, isolated locations known as Availability Zones. Edge locations are for AWS to cache contents, e.g. if a user in Sydney requires content from New York region, the content will be first downloaded from New York and stored in Sydney, so if the same content is requested again it will be available from the Sydney region. Currently there are more than 150 edge locations around the world. VPC A Virtual Private Cloud (VPC) is a virtual network dedicated to a single AWS account. It is logically isolated from other virtual networks in the AWS cloud, providing compute resources with security and robust networking functionality. IAM (Identity Management Service) Users, end users such as employees of an organisation. Groups, a collection of users, each user in the group inherits the permissions of the group. Policies, json format documents describe the permissions that user, group or role can or cannot do. Roles can be assigned to AWS resources. IAM created users are global and not tied down to a region, same for group and role. A root account is the one that created AWS account, it has complete admin access. When a user is created, it has no permissions by default. S3 Basics When uploading a file, a HTTP 200 code is received to indicate upload is successful. Creating a bucket will generate a unique HTTPS link. Object-based, think of objects as files. You cannot use it as databases. It can consist of the following: Key, the name of the object. Value, the data, made up of sequence of bytes. Version ID, S3 allows you to have multiple versions of a file. Metadata, data about the data you store. Subresources: Access Control Lists. Torrent. Data Consistency Read after write consistency, if you write a file and immediately read after, you will be able to view the data. Eventual consistency, If you update an existing file or delete a file, and read immediately, you may get the old version or you may not. Basically changes to existing objects can take a bit of time to propagate. Features Tiered storage S3 standard, 99.99% availability, 99.99..% (11 9s\u2019) durability. Stored redundantly across multiple devices in multiple facilities, designed to sustain the loss of 2 facilities. S3 IA (infrequently accessed), for data access less frequently, but require rapid access when needed. Cost less than S3, but charged on retrieval. S3 One Zone IA, same as IA, low cost, do not require multiple available availability zone. S3 Intelligent Tiering, using ML to determine tier automatically based on how your data is accessed. Introduced in 2018. S3 Glacier, secure, durable, and low cost storage, retrieval time varies from minutes to hours. S3 Glacier Deep Archive, lowest cost storage, retrieval time of 12 hours is acceptable. Lifecycle management. Versioning. Encryption. MFA authentication for deleting objects, data protection. Secure your data using Access Control Lists and Bucket Policies, basically permissions for objects and buckets. Cross region replication, upload in one region data will be replicated to other regions. S3 Transfer Acceleration, takes advantage of CloudFront global distributed edge locations, data is routed to S3 using optimised network path. Read S3 FAQ before taking the exam . Security and Encryption Security is achieved by using Access Control Lists (for files) and Bucket Policies (for buckets). Encryption in transit, using SSL/TLS, like HTTPS. Encryption at rest, server side encrypted, is achieved using: S3 managed key, used to encrypt and decrypt objects. SSE-S3 (server side encryption S3), managed by Amazon. AWS Key Management Service. SSE-KMS. SSE-C with customer provided key. Client side encryption, you upload encrypted objects to S3. Versioning Once the bucket with the versioning turned on, it cannot be turned off. Has MFA delete capability. Integrates with Lifecycle rules. Upload a newer version of a file changes it\u2019s permission to default access denied. With versioning turned on the size of your bucket will grow exponentially. Delete versioned file only puts a delete marker to the file, versioned files can be restored by deleting the delete marker. Individual versions can be deleted. Lifecycle Management Automates moving objects between the different storage tiers. Can be used with versioning. Can be applied to current version and previous versions. Cross Region Replication Region must be unique. Existing objects on source bucket won\u2019t be replicated. Only new files are replicated. Delete marker won\u2019t be replicated. Delete individual files or delete marker won\u2019t be replicated. CloudFront CloudFront is a global service, not based on region. You can restrict access using signed URLs. CDN, content delivery network, content cached on Edge location, initial content request will be fetched from the origin, but subsequent content request will be faster. Edge Location, the location where the content will be cached, this is separate the a region/AZ. Origin, the location stores the content CDN will distribute. This could be a S3 bucket, EC2 instance, Elastic load balancer, or Route53. Distribution, a collection of ELs. Web Distribution, used for websites. RTMP, used for video streaming. ELs are not just for reading, but also writing, e.g. S3 accelerated transfer uses CloudFront. Objects are cached for TTL (time to live in seconds), which can be configured. You can clear cache, but you will be charged for that. Snowball Used to transfer large amounts of data. Can be imported to S3. Can be exported from S3. Storage Gateway File Gateway, for flat files stored directly to S3. Volume Gateways Stored volumes, entire dataset is stored on site and synchronously backed up to S3. Cached volumes, entire dataset is stored on S3, only the most frequently accessed data will be stored on site. Gateway Access Tape Library, options available if backing up tape libraries. EC2 Amazon Elastic Compute Cloud is a web service that provides resizable compute capacity in the cloud. Reduce the time required to boot new server instances to minutes, quickly scale capacity, both up and down. * Termination Protection is turned off by default. * Root volume is where the OS is installed. * EBS-backed instance, the default action is for the root EBS volume to be deleted when the instance is terminated. * EBS root volumes cannot be encrypted. Additional volumes can be encrypted. Pricing Models On Demand allows you to pay a fixed rate by hours or seconds, with no commitment. Low cost without any upfront payment or long term commitment. Applications with short term, spiky, or unpredictable workloads that cannot be interrupted. Application being developed or tested on EC2 for the first time. Reserved provides you with a capacity reservation, and offer a significant discount on hourly charge for an instance. Contract terms are 1 year or 3 years. Steady state or predictable usage. Require reserved capacity. Users are able to make upfront payments to reduce their computing cost even further. Spot enables you to bid whatever price you want for instance capacity, works like a stock market. Flexible start end times. Applications that are only feasible at very local computer prices. Users with urgent computing needs for large amounts of additional capacity. Dedicated Useful for regulatory requirements not support multi-tenant virtualization. Licensing doesn\u2019t support multi-tenancy or cloud deployments. Note you won\u2019t be charged for partial hours of usage if your spot instance is terminated. If you terminate the instance yourself, you will be charged for any hour in which the instance ran. EC2 Security Groups Every time you make a change to the security group the change will take effect immediately. When an Inbound rule is created a corresponding Outbound rule is created for the inbound rule. It is Stateful. Everything is blocked by default. All outbound traffic us allowed. You can have any number of EC2 instances within a security group. You can have multiple security groups attached to EC2 instances. You cannot block specific IP addresses, you can only specify allow rules not deny rules. EBS Virtual hard disk, snapshots exist on S3. Snapshots are incremental, this means that only the blocks that have changed since your last snapshot are moved to S3. You should stop the instance before taking the snapshot, but you can take a snapshot while it is running. You can create AMIs from both volumes and snapshots. EBS volume size and type can be changed on the fly. Wherever your EC2 instance is, the volume will alway be in the same region. If EC2 instance is deleted the root device is deleted too by default. However additional volumes attached to the instance stay persist. To move an EC2 volume from one AZ to another, take a snapshot of it, create an AMI from the snapshot and then use the AMI to launch the EC2 instance in a new AZ. SImilarly the EC2 instance can be moved to another region. Instance Store Volumes Instance Store Volumes (Ephemeral) cannot be stopped, if the underlying hosts fails, you lose your data. You can reboot EBS or ISV, data will not be lost. Both root volumes will be deleted on termination. It is mainly used for constant changing data, since ISVs are closer to physical memory and much faster than EBS. Encrypted Root Device Snapshots of encrypted volumes are encrypted automatically. Volumes restored from encrypted snapshots are encrypted automatically. Snapshots can only be shared if they are unencrypted. Snapshots can be shared with other AWS accounts or made public. You can now encrypt root device volumes upon creation of the ECs instance. On existing volume, create a snapshot, copy the snapshot enable it with encryption, then create an AMI. New instances can be launched using the encrypted AMI. You cannot take an encrypted AMI and launch it as an unencrypted instance. CloudWatch CloudWatch is used to monitor performance. CloudWatch can monitor most of AWS as well as your applications. CloudWatch with EC2 will monitor events every 5 minutes by default and you can have 1 minute intervals by turning on detailed monitoring. CloudWatch alarms can trigger notifications. CloudWatch is all about performance and CloudTrial is all about auditing. IAM Roles with EC2 Roles are more secure than storing your access key and secret access key on EC2 instance. Roles are easier to manage, and can be assigned to an EC2 instance after it is created. Roles are universal, you can use them in any region. EFS (Elastic File System)","title":"AWS solution architect associate"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#aws-solution-architect-associate","text":"","title":"AWS solution architect associate"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#high-level-services","text":"","title":"High Level Services"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#difference-between-regions-availability-zones-and-edge-location","text":"Availability zone is basically a data centre, or multiple data centres are close to each other. Region is a geographical area, contains two or more availability zones. Each region is a separate geographic area. * Each region has multiple, isolated locations known as Availability Zones. Edge locations are for AWS to cache contents, e.g. if a user in Sydney requires content from New York region, the content will be first downloaded from New York and stored in Sydney, so if the same content is requested again it will be available from the Sydney region. Currently there are more than 150 edge locations around the world.","title":"Difference between Regions, Availability Zones and Edge Location"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#vpc","text":"A Virtual Private Cloud (VPC) is a virtual network dedicated to a single AWS account. It is logically isolated from other virtual networks in the AWS cloud, providing compute resources with security and robust networking functionality.","title":"VPC"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#iam-identity-management-service","text":"Users, end users such as employees of an organisation. Groups, a collection of users, each user in the group inherits the permissions of the group. Policies, json format documents describe the permissions that user, group or role can or cannot do. Roles can be assigned to AWS resources. IAM created users are global and not tied down to a region, same for group and role. A root account is the one that created AWS account, it has complete admin access. When a user is created, it has no permissions by default.","title":"IAM (Identity Management Service)"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#s3","text":"","title":"S3"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#basics","text":"When uploading a file, a HTTP 200 code is received to indicate upload is successful. Creating a bucket will generate a unique HTTPS link. Object-based, think of objects as files. You cannot use it as databases. It can consist of the following: Key, the name of the object. Value, the data, made up of sequence of bytes. Version ID, S3 allows you to have multiple versions of a file. Metadata, data about the data you store. Subresources: Access Control Lists. Torrent.","title":"Basics"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#data-consistency","text":"Read after write consistency, if you write a file and immediately read after, you will be able to view the data. Eventual consistency, If you update an existing file or delete a file, and read immediately, you may get the old version or you may not. Basically changes to existing objects can take a bit of time to propagate.","title":"Data Consistency"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#features","text":"Tiered storage S3 standard, 99.99% availability, 99.99..% (11 9s\u2019) durability. Stored redundantly across multiple devices in multiple facilities, designed to sustain the loss of 2 facilities. S3 IA (infrequently accessed), for data access less frequently, but require rapid access when needed. Cost less than S3, but charged on retrieval. S3 One Zone IA, same as IA, low cost, do not require multiple available availability zone. S3 Intelligent Tiering, using ML to determine tier automatically based on how your data is accessed. Introduced in 2018. S3 Glacier, secure, durable, and low cost storage, retrieval time varies from minutes to hours. S3 Glacier Deep Archive, lowest cost storage, retrieval time of 12 hours is acceptable. Lifecycle management. Versioning. Encryption. MFA authentication for deleting objects, data protection. Secure your data using Access Control Lists and Bucket Policies, basically permissions for objects and buckets. Cross region replication, upload in one region data will be replicated to other regions. S3 Transfer Acceleration, takes advantage of CloudFront global distributed edge locations, data is routed to S3 using optimised network path. Read S3 FAQ before taking the exam .","title":"Features"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#security-and-encryption","text":"Security is achieved by using Access Control Lists (for files) and Bucket Policies (for buckets). Encryption in transit, using SSL/TLS, like HTTPS. Encryption at rest, server side encrypted, is achieved using: S3 managed key, used to encrypt and decrypt objects. SSE-S3 (server side encryption S3), managed by Amazon. AWS Key Management Service. SSE-KMS. SSE-C with customer provided key. Client side encryption, you upload encrypted objects to S3.","title":"Security and Encryption"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#versioning","text":"Once the bucket with the versioning turned on, it cannot be turned off. Has MFA delete capability. Integrates with Lifecycle rules. Upload a newer version of a file changes it\u2019s permission to default access denied. With versioning turned on the size of your bucket will grow exponentially. Delete versioned file only puts a delete marker to the file, versioned files can be restored by deleting the delete marker. Individual versions can be deleted.","title":"Versioning"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#lifecycle-management","text":"Automates moving objects between the different storage tiers. Can be used with versioning. Can be applied to current version and previous versions.","title":"Lifecycle Management"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#cross-region-replication","text":"Region must be unique. Existing objects on source bucket won\u2019t be replicated. Only new files are replicated. Delete marker won\u2019t be replicated. Delete individual files or delete marker won\u2019t be replicated.","title":"Cross Region Replication"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#cloudfront","text":"CloudFront is a global service, not based on region. You can restrict access using signed URLs. CDN, content delivery network, content cached on Edge location, initial content request will be fetched from the origin, but subsequent content request will be faster. Edge Location, the location where the content will be cached, this is separate the a region/AZ. Origin, the location stores the content CDN will distribute. This could be a S3 bucket, EC2 instance, Elastic load balancer, or Route53. Distribution, a collection of ELs. Web Distribution, used for websites. RTMP, used for video streaming. ELs are not just for reading, but also writing, e.g. S3 accelerated transfer uses CloudFront. Objects are cached for TTL (time to live in seconds), which can be configured. You can clear cache, but you will be charged for that.","title":"CloudFront"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#snowball","text":"Used to transfer large amounts of data. Can be imported to S3. Can be exported from S3.","title":"Snowball"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#storage-gateway","text":"File Gateway, for flat files stored directly to S3. Volume Gateways Stored volumes, entire dataset is stored on site and synchronously backed up to S3. Cached volumes, entire dataset is stored on S3, only the most frequently accessed data will be stored on site. Gateway Access Tape Library, options available if backing up tape libraries.","title":"Storage Gateway"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#ec2","text":"Amazon Elastic Compute Cloud is a web service that provides resizable compute capacity in the cloud. Reduce the time required to boot new server instances to minutes, quickly scale capacity, both up and down. * Termination Protection is turned off by default. * Root volume is where the OS is installed. * EBS-backed instance, the default action is for the root EBS volume to be deleted when the instance is terminated. * EBS root volumes cannot be encrypted. Additional volumes can be encrypted.","title":"EC2"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#pricing-models","text":"On Demand allows you to pay a fixed rate by hours or seconds, with no commitment. Low cost without any upfront payment or long term commitment. Applications with short term, spiky, or unpredictable workloads that cannot be interrupted. Application being developed or tested on EC2 for the first time. Reserved provides you with a capacity reservation, and offer a significant discount on hourly charge for an instance. Contract terms are 1 year or 3 years. Steady state or predictable usage. Require reserved capacity. Users are able to make upfront payments to reduce their computing cost even further. Spot enables you to bid whatever price you want for instance capacity, works like a stock market. Flexible start end times. Applications that are only feasible at very local computer prices. Users with urgent computing needs for large amounts of additional capacity. Dedicated Useful for regulatory requirements not support multi-tenant virtualization. Licensing doesn\u2019t support multi-tenancy or cloud deployments. Note you won\u2019t be charged for partial hours of usage if your spot instance is terminated. If you terminate the instance yourself, you will be charged for any hour in which the instance ran.","title":"Pricing Models"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#ec2-security-groups","text":"Every time you make a change to the security group the change will take effect immediately. When an Inbound rule is created a corresponding Outbound rule is created for the inbound rule. It is Stateful. Everything is blocked by default. All outbound traffic us allowed. You can have any number of EC2 instances within a security group. You can have multiple security groups attached to EC2 instances. You cannot block specific IP addresses, you can only specify allow rules not deny rules.","title":"EC2 Security Groups"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#ebs","text":"Virtual hard disk, snapshots exist on S3. Snapshots are incremental, this means that only the blocks that have changed since your last snapshot are moved to S3. You should stop the instance before taking the snapshot, but you can take a snapshot while it is running. You can create AMIs from both volumes and snapshots. EBS volume size and type can be changed on the fly. Wherever your EC2 instance is, the volume will alway be in the same region. If EC2 instance is deleted the root device is deleted too by default. However additional volumes attached to the instance stay persist. To move an EC2 volume from one AZ to another, take a snapshot of it, create an AMI from the snapshot and then use the AMI to launch the EC2 instance in a new AZ. SImilarly the EC2 instance can be moved to another region.","title":"EBS"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#instance-store-volumes","text":"Instance Store Volumes (Ephemeral) cannot be stopped, if the underlying hosts fails, you lose your data. You can reboot EBS or ISV, data will not be lost. Both root volumes will be deleted on termination. It is mainly used for constant changing data, since ISVs are closer to physical memory and much faster than EBS.","title":"Instance Store Volumes"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#encrypted-root-device","text":"Snapshots of encrypted volumes are encrypted automatically. Volumes restored from encrypted snapshots are encrypted automatically. Snapshots can only be shared if they are unencrypted. Snapshots can be shared with other AWS accounts or made public. You can now encrypt root device volumes upon creation of the ECs instance. On existing volume, create a snapshot, copy the snapshot enable it with encryption, then create an AMI. New instances can be launched using the encrypted AMI. You cannot take an encrypted AMI and launch it as an unencrypted instance.","title":"Encrypted Root Device"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#cloudwatch","text":"CloudWatch is used to monitor performance. CloudWatch can monitor most of AWS as well as your applications. CloudWatch with EC2 will monitor events every 5 minutes by default and you can have 1 minute intervals by turning on detailed monitoring. CloudWatch alarms can trigger notifications. CloudWatch is all about performance and CloudTrial is all about auditing.","title":"CloudWatch"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#iam-roles-with-ec2","text":"Roles are more secure than storing your access key and secret access key on EC2 instance. Roles are easier to manage, and can be assigned to an EC2 instance after it is created. Roles are universal, you can use them in any region.","title":"IAM Roles with EC2"},{"location":"notes/aws/AWS%20solution%20architect%20associate/#efs-elastic-file-system","text":"","title":"EFS (Elastic File System)"},{"location":"notes/general/Quick%20notes/","text":"Quick notes Ideas or definitions that are heard but not sure what they mean. Imperative vs declarative With imperative programming, you tell the compiler what you want to happen, step by step. List < int > collection = new List < int > { 1 , 2 , 3 , 4 , 5 }; List < int > results = new List < int >(); foreach ( var num in collection ) { if ( num % 2 != 0 ) results . Add ( num ); } With declarative, you write code that describes what you want but not necessarily how to get it (declare your desired result, but now the step-by-step). var results = collection . Where ( num => num % 2 != 0 ); LINQ operations are considered declarative, here we say \"give me everything that is odd\", not \"step through the collection, check this item, if it is odd then add it to the collection\". A simple example in python. # Declarative small_nums = [ x for x in range ( 20 ) if x < 5 ] # Imperative small_nums = [] for i in range ( 20 ): if i < 5 : small_nums . append ( i ) Nullish assignments The nullish coalescing operator is evaluated left to right, it is tested for possible short-circuit evaluation using the following rule: (some expression that is neither null nor undefined) ?? expr is short-circuit evaluated to the left-hand side expression if left-hand side proves to be either null or undefined . It is equivalent to x ?? (x = y); . x = 10 > 10 y = 20 > 20 x ?? y > 10 x = undefined > undefined x ??= y > 20 x ??= 30 > 20","title":"Quick notes"},{"location":"notes/general/Quick%20notes/#quick-notes","text":"Ideas or definitions that are heard but not sure what they mean.","title":"Quick notes"},{"location":"notes/general/Quick%20notes/#imperative-vs-declarative","text":"With imperative programming, you tell the compiler what you want to happen, step by step. List < int > collection = new List < int > { 1 , 2 , 3 , 4 , 5 }; List < int > results = new List < int >(); foreach ( var num in collection ) { if ( num % 2 != 0 ) results . Add ( num ); } With declarative, you write code that describes what you want but not necessarily how to get it (declare your desired result, but now the step-by-step). var results = collection . Where ( num => num % 2 != 0 ); LINQ operations are considered declarative, here we say \"give me everything that is odd\", not \"step through the collection, check this item, if it is odd then add it to the collection\". A simple example in python. # Declarative small_nums = [ x for x in range ( 20 ) if x < 5 ] # Imperative small_nums = [] for i in range ( 20 ): if i < 5 : small_nums . append ( i )","title":"Imperative vs declarative"},{"location":"notes/general/Quick%20notes/#nullish-assignments","text":"The nullish coalescing operator is evaluated left to right, it is tested for possible short-circuit evaluation using the following rule: (some expression that is neither null nor undefined) ?? expr is short-circuit evaluated to the left-hand side expression if left-hand side proves to be either null or undefined . It is equivalent to x ?? (x = y); . x = 10 > 10 y = 20 > 20 x ?? y > 10 x = undefined > undefined x ??= y > 20 x ??= 30 > 20","title":"Nullish assignments"},{"location":"notes/microservices/Building%20microservices%20with%20AWS%20Lambda/","text":"Building Microservices with AWS Lambda AWS re:Invent 2019: [REPEAT 1] Building microservices with AWS Lambda (SVS343-R1) Request Routing Handles request routing inside Lambda function: Security constructs applied to whole function Performance settings treated as the whole Limited amount of application size Limited duration The \u201cLambda-lith\u201d can grow too in complexity, might need to rethink the whole logic routing model Fewer cold starts? AWS needs to find available resources bootstrap the environment then starts executing the code Fewer security constructs (IAM roles and policies)? Take out the code that could be running in Lambda or container API Gateway Using the benefits of API Gateway Better compatibility with AWS\u2019s tools Better security granularity Better performance granularity Async vs Sync The time spent to try making a process async will pay for itself in you gaining a deeper understanding of what is really happening with your data. Sync Approach Who handles failure and retry? Who owns the retry? For how long? This creates a \u201ctight coupling\u201d where failures are hard to recover from Async Approach Service A immediately returns Client has to handle talking to Service B Anatomy of a Serverless Application Note that in a microservices architecture, each service should persist its own states and data. Each Lambda function's configuration can be stored as environment variables and be injected at deployment time. A Lambda layer can be deployed to avoid code duplication, each Lambda function will have access to. Layer can be anything, dependencies, training data, configuration files.","title":"Building Microservices with AWS Lambda"},{"location":"notes/microservices/Building%20microservices%20with%20AWS%20Lambda/#building-microservices-with-aws-lambda","text":"AWS re:Invent 2019: [REPEAT 1] Building microservices with AWS Lambda (SVS343-R1)","title":"Building Microservices with AWS Lambda"},{"location":"notes/microservices/Building%20microservices%20with%20AWS%20Lambda/#request-routing","text":"Handles request routing inside Lambda function: Security constructs applied to whole function Performance settings treated as the whole Limited amount of application size Limited duration The \u201cLambda-lith\u201d can grow too in complexity, might need to rethink the whole logic routing model Fewer cold starts? AWS needs to find available resources bootstrap the environment then starts executing the code Fewer security constructs (IAM roles and policies)? Take out the code that could be running in Lambda or container API Gateway Using the benefits of API Gateway Better compatibility with AWS\u2019s tools Better security granularity Better performance granularity","title":"Request Routing"},{"location":"notes/microservices/Building%20microservices%20with%20AWS%20Lambda/#async-vs-sync","text":"The time spent to try making a process async will pay for itself in you gaining a deeper understanding of what is really happening with your data. Sync Approach Who handles failure and retry? Who owns the retry? For how long? This creates a \u201ctight coupling\u201d where failures are hard to recover from Async Approach Service A immediately returns Client has to handle talking to Service B","title":"Async vs Sync"},{"location":"notes/microservices/Building%20microservices%20with%20AWS%20Lambda/#anatomy-of-a-serverless-application","text":"Note that in a microservices architecture, each service should persist its own states and data. Each Lambda function's configuration can be stored as environment variables and be injected at deployment time. A Lambda layer can be deployed to avoid code duplication, each Lambda function will have access to. Layer can be anything, dependencies, training data, configuration files.","title":"Anatomy of a Serverless Application"},{"location":"notes/microservices/Microservices%20architecture/","text":"Microservices architecture Microservices architecture style - Azure Application Architecture Guide Design principles of microservices Microservices are small independent loosely coupled, and can be maintained by different teams Each service is a separate codebase and can be deployed independently. Services are responsible for maintaining its own state and data persistence Services communicate with each other by using well defined APIs, internal implementation details should be hidden Services don\u2019t need to share the same technology stack, libraries, or frameworks API gateway is the entry point for clients, instead of calling services directly, clients call the API gateway, which forwards to the appropriate services. API gateway should not have any domain knowledge, it should only handle request routing Pros Agility. Services are deployed independently, it\u2019s easier to manage bug fixes and releases Scalability. Services can be scaled independently without the need to scale the whole application Fault tolerant. If a service is unavailable, it won\u2019t disrupt the whole application, provided the upstream services are designed to handle fault correctly Data isolation. Easier to perform schema updates, where traditional monolithic application it can be very difficult because many parts of the application may touch the same data Cons Complexity. Each service may be simpler but the whole application as a whole is more complex Testing. Service dependencies can be difficult to test Network latency Data integrity. With microservices are responsible for its own data persistence, embrace eventual consistency where possible Design patterns Gateway aggregation Use a gateway to aggregate multiple individual requests into a single request. This pattern is useful when a client must make multiple calls to different backend systems to perform an operation. Problem To perform a single task, a client may have to make multiple calls to various backend services. When any new feature or service is added to the application, additional requests are needed, further increasing resource requirements and network calls. In the following diagram, the client sends requests to each service (1,2,3). Each service processes the request and sends the response back to the application (4,5,6). While each request may be done in parallel, the application must send, wait, and process data for each request, all on separate connections, increasing the chance of failure. Solution In the following diagram, the application sends a request to the gateway (1). The request contains a package of additional requests. The gateway decomposes these and processes each request by sending it to the relevant service (2). Each service returns a response to the gateway (3). The gateway combines the responses from each service and sends the response to the application (4). The application makes a single request and receives only a single response from the gateway. Considerations The gateway should not introduce coupling with service. The gateway service may introduce a single point of failure. The gateway service may introduce bottleneck. Instead of building aggregation into the gateway, consider placing an aggregation service behind the gateway. Request aggregation will likely have different resource requirements than other services in the gateway and may impact the gateway's routing and offloading functionality. Ambassador Create helper services that send network requests on behalf of a consumer service or application. An ambassador service can be thought of as an out-of-process proxy that is co-located with the client. Deploy the proxy on the same host environment as your application to allow control over routing, resiliency, security features, and to avoid any host-related access restrictions (CORS). Circuit Breaker Handle faults that might take a variable amount of time to recover from, when connecting to a remote service or resource. This can improve the stability and resiliency of an application. The purpose of the Circuit Breaker pattern is different than the Retry pattern. The Retry pattern enables an application to retry an operation in the expectation that it'll succeed. The Circuit Breaker pattern prevents an application from performing an operation that is likely to fail. An application can combine these two patterns by using the Retry pattern to invoke an operation through a circuit breaker. However, the retry logic should be sensitive to any exceptions returned by the circuit breaker and abandon retry attempts if the circuit breaker indicates that a fault is not transient. A Circuit Breaker shall act as a proxy for operations that might fail. It should monitor the number of recent failures and use this information to decide whether to allow operations to proceed. A Circuit Breaker has three states: Closed , Open and Half-Open . Closed: The request from the application is routed to the operation. The proxy maintains a counter of recent failed operations and increment the failed counter. If the number of failed operations exceeds the specified threshold within a given period of time, the proxy is in Open state. At this point the proxy starts a timeout timer, and when the timeout expires, it enters Half-Open state. The purpose of timeout is to give the system time to fix the problem before allow the system to perform the operations again. Open: The operation from the application fails immediately and error/exception is returned. Half-Open: A limited number of requests are allowed to perform the operation. If these requests are successful, it is assumed that the system is recovered from previous failure and the proxy enters Closed state. If any requests failed, the proxy enters Open state and restarts a timeout timer. The Half-Open state is useful to prevent a recovering service suddenly being flooded with request. The Circuit Breaker pattern provides stability while the system recovers from a failure and minimizes the impact on performance. It can help to maintain the response time of the system by quickly rejecting a request for an operation that's likely to fail, rather than waiting for the operation to time out, or never return. interface ICircuitBreakerStateStore { CircuitBreakerStateEnum State { get ; } Exception LastException { get ; } DateTime LastStateChangedDateUtc { get ; } void Trip ( Exception ex ); void Reset (); void HalfOpen (); bool IsClosed { get ; } } public class CircuitBreaker { private readonly ICircuitBreakerStateStore stateStore = CircuitBreakerStateStoreFactory . GetCircuitBreakerStateStore (); private readonly object halfOpenSyncObject = new object (); public bool IsClosed { get { return stateStore . IsClosed ; } } public bool IsOpen { get { return ! IsClosed ; } } public void ExecuteAction ( Action action ) { if ( IsOpen ) { // The circuit breaker is Open. Check if the Open timeout has expired. // If it has, set the state to HalfOpen. Another approach might be to // check for the HalfOpen state that had be set by some other operation. if ( stateStore . LastStateChangedDateUtc + OpenToHalfOpenWaitTime < DateTime . UtcNow ) { // The Open timeout has expired. Allow one operation to execute. Note that, in // this example, the circuit breaker is set to HalfOpen after being // in the Open state for some period of time. An alternative would be to set // this using some other approach such as a timer, test method, manually, and // so on, and check the state here to determine how to handle execution // of the action. // Limit the number of threads to be executed when the breaker is HalfOpen. // An alternative would be to use a more complex approach to determine which // threads or how many are allowed to execute, or to execute a simple test // method instead. bool lockTaken = false ; try { Monitor . TryEnter ( halfOpenSyncObject , ref lockTaken ); if ( lockTaken ) { // Set the circuit breaker state to HalfOpen. stateStore . HalfOpen (); // Attempt the operation. action (); // If this action succeeds, reset the state and allow other operations. // In reality, instead of immediately returning to the Closed state, a counter // here would record the number of successful operations and return the // circuit breaker to the Closed state only after a specified number succeed. this . stateStore . Reset (); return ; } } catch ( Exception ex ) { // If there's still an exception, trip the breaker again immediately. this . stateStore . Trip ( ex ); // Throw the exception so that the caller knows which exception occurred. throw ; } finally { if ( lockTaken ) { Monitor . Exit ( halfOpenSyncObject ); } } } // The Open timeout hasn't yet expired. Throw a CircuitBreakerOpen exception to // inform the caller that the call was not actually attempted, // and return the most recent exception received. throw new CircuitBreakerOpenException ( stateStore . LastException ); // The circuit breaker is Open. } // The circuit breaker is Closed, execute the action. try { action (); } catch ( Exception ex ) { // If an exception still occurs here, simply // retrip the breaker immediately. this . TrackException ( ex ); // Throw the exception so that the caller can tell // the type of exception that was thrown. throw ; } } private void TrackException ( Exception ex ) { // For simplicity in this example, open the circuit breaker on the first exception. // In reality this would be more complex. A certain type of exception, such as one // that indicates a service is offline, might trip the circuit breaker immediately. // Alternatively it might count exceptions locally or across multiple instances and // use this value over time, or the exception/success ratio based on the exception // types, to open the circuit breaker. this . stateStore . Trip ( ex ); } } To use a CircuitBreaker object to protect an operation, an application creates an instance of the CircuitBreaker class and invokes the ExecuteAction method, specifying the operation to be performed as the parameter. The application should be prepared to catch the CircuitBreakerOpenException exception if the operation fails because the circuit breaker is open. var breaker = new CircuitBreaker (); try { breaker . ExecuteAction (() => { // Operation protected by the circuit breaker. ... }); } catch ( CircuitBreakerOpenException ex ) { // Perform some different action when the breaker is open. // Last exception details are in the inner exception. ... } catch ( Exception ex ) { ... }","title":"Microservices architecture"},{"location":"notes/microservices/Microservices%20architecture/#microservices-architecture","text":"Microservices architecture style - Azure Application Architecture Guide","title":"Microservices architecture"},{"location":"notes/microservices/Microservices%20architecture/#design-principles-of-microservices","text":"Microservices are small independent loosely coupled, and can be maintained by different teams Each service is a separate codebase and can be deployed independently. Services are responsible for maintaining its own state and data persistence Services communicate with each other by using well defined APIs, internal implementation details should be hidden Services don\u2019t need to share the same technology stack, libraries, or frameworks API gateway is the entry point for clients, instead of calling services directly, clients call the API gateway, which forwards to the appropriate services. API gateway should not have any domain knowledge, it should only handle request routing","title":"Design principles of microservices"},{"location":"notes/microservices/Microservices%20architecture/#pros","text":"Agility. Services are deployed independently, it\u2019s easier to manage bug fixes and releases Scalability. Services can be scaled independently without the need to scale the whole application Fault tolerant. If a service is unavailable, it won\u2019t disrupt the whole application, provided the upstream services are designed to handle fault correctly Data isolation. Easier to perform schema updates, where traditional monolithic application it can be very difficult because many parts of the application may touch the same data","title":"Pros"},{"location":"notes/microservices/Microservices%20architecture/#cons","text":"Complexity. Each service may be simpler but the whole application as a whole is more complex Testing. Service dependencies can be difficult to test Network latency Data integrity. With microservices are responsible for its own data persistence, embrace eventual consistency where possible","title":"Cons"},{"location":"notes/microservices/Microservices%20architecture/#design-patterns","text":"","title":"Design patterns"},{"location":"notes/microservices/Microservices%20architecture/#gateway-aggregation","text":"Use a gateway to aggregate multiple individual requests into a single request. This pattern is useful when a client must make multiple calls to different backend systems to perform an operation.","title":"Gateway aggregation"},{"location":"notes/microservices/Microservices%20architecture/#problem","text":"To perform a single task, a client may have to make multiple calls to various backend services. When any new feature or service is added to the application, additional requests are needed, further increasing resource requirements and network calls. In the following diagram, the client sends requests to each service (1,2,3). Each service processes the request and sends the response back to the application (4,5,6). While each request may be done in parallel, the application must send, wait, and process data for each request, all on separate connections, increasing the chance of failure.","title":"Problem"},{"location":"notes/microservices/Microservices%20architecture/#solution","text":"In the following diagram, the application sends a request to the gateway (1). The request contains a package of additional requests. The gateway decomposes these and processes each request by sending it to the relevant service (2). Each service returns a response to the gateway (3). The gateway combines the responses from each service and sends the response to the application (4). The application makes a single request and receives only a single response from the gateway.","title":"Solution"},{"location":"notes/microservices/Microservices%20architecture/#considerations","text":"The gateway should not introduce coupling with service. The gateway service may introduce a single point of failure. The gateway service may introduce bottleneck. Instead of building aggregation into the gateway, consider placing an aggregation service behind the gateway. Request aggregation will likely have different resource requirements than other services in the gateway and may impact the gateway's routing and offloading functionality.","title":"Considerations"},{"location":"notes/microservices/Microservices%20architecture/#ambassador","text":"Create helper services that send network requests on behalf of a consumer service or application. An ambassador service can be thought of as an out-of-process proxy that is co-located with the client. Deploy the proxy on the same host environment as your application to allow control over routing, resiliency, security features, and to avoid any host-related access restrictions (CORS).","title":"Ambassador"},{"location":"notes/microservices/Microservices%20architecture/#circuit-breaker","text":"Handle faults that might take a variable amount of time to recover from, when connecting to a remote service or resource. This can improve the stability and resiliency of an application. The purpose of the Circuit Breaker pattern is different than the Retry pattern. The Retry pattern enables an application to retry an operation in the expectation that it'll succeed. The Circuit Breaker pattern prevents an application from performing an operation that is likely to fail. An application can combine these two patterns by using the Retry pattern to invoke an operation through a circuit breaker. However, the retry logic should be sensitive to any exceptions returned by the circuit breaker and abandon retry attempts if the circuit breaker indicates that a fault is not transient. A Circuit Breaker shall act as a proxy for operations that might fail. It should monitor the number of recent failures and use this information to decide whether to allow operations to proceed. A Circuit Breaker has three states: Closed , Open and Half-Open . Closed: The request from the application is routed to the operation. The proxy maintains a counter of recent failed operations and increment the failed counter. If the number of failed operations exceeds the specified threshold within a given period of time, the proxy is in Open state. At this point the proxy starts a timeout timer, and when the timeout expires, it enters Half-Open state. The purpose of timeout is to give the system time to fix the problem before allow the system to perform the operations again. Open: The operation from the application fails immediately and error/exception is returned. Half-Open: A limited number of requests are allowed to perform the operation. If these requests are successful, it is assumed that the system is recovered from previous failure and the proxy enters Closed state. If any requests failed, the proxy enters Open state and restarts a timeout timer. The Half-Open state is useful to prevent a recovering service suddenly being flooded with request. The Circuit Breaker pattern provides stability while the system recovers from a failure and minimizes the impact on performance. It can help to maintain the response time of the system by quickly rejecting a request for an operation that's likely to fail, rather than waiting for the operation to time out, or never return. interface ICircuitBreakerStateStore { CircuitBreakerStateEnum State { get ; } Exception LastException { get ; } DateTime LastStateChangedDateUtc { get ; } void Trip ( Exception ex ); void Reset (); void HalfOpen (); bool IsClosed { get ; } } public class CircuitBreaker { private readonly ICircuitBreakerStateStore stateStore = CircuitBreakerStateStoreFactory . GetCircuitBreakerStateStore (); private readonly object halfOpenSyncObject = new object (); public bool IsClosed { get { return stateStore . IsClosed ; } } public bool IsOpen { get { return ! IsClosed ; } } public void ExecuteAction ( Action action ) { if ( IsOpen ) { // The circuit breaker is Open. Check if the Open timeout has expired. // If it has, set the state to HalfOpen. Another approach might be to // check for the HalfOpen state that had be set by some other operation. if ( stateStore . LastStateChangedDateUtc + OpenToHalfOpenWaitTime < DateTime . UtcNow ) { // The Open timeout has expired. Allow one operation to execute. Note that, in // this example, the circuit breaker is set to HalfOpen after being // in the Open state for some period of time. An alternative would be to set // this using some other approach such as a timer, test method, manually, and // so on, and check the state here to determine how to handle execution // of the action. // Limit the number of threads to be executed when the breaker is HalfOpen. // An alternative would be to use a more complex approach to determine which // threads or how many are allowed to execute, or to execute a simple test // method instead. bool lockTaken = false ; try { Monitor . TryEnter ( halfOpenSyncObject , ref lockTaken ); if ( lockTaken ) { // Set the circuit breaker state to HalfOpen. stateStore . HalfOpen (); // Attempt the operation. action (); // If this action succeeds, reset the state and allow other operations. // In reality, instead of immediately returning to the Closed state, a counter // here would record the number of successful operations and return the // circuit breaker to the Closed state only after a specified number succeed. this . stateStore . Reset (); return ; } } catch ( Exception ex ) { // If there's still an exception, trip the breaker again immediately. this . stateStore . Trip ( ex ); // Throw the exception so that the caller knows which exception occurred. throw ; } finally { if ( lockTaken ) { Monitor . Exit ( halfOpenSyncObject ); } } } // The Open timeout hasn't yet expired. Throw a CircuitBreakerOpen exception to // inform the caller that the call was not actually attempted, // and return the most recent exception received. throw new CircuitBreakerOpenException ( stateStore . LastException ); // The circuit breaker is Open. } // The circuit breaker is Closed, execute the action. try { action (); } catch ( Exception ex ) { // If an exception still occurs here, simply // retrip the breaker immediately. this . TrackException ( ex ); // Throw the exception so that the caller can tell // the type of exception that was thrown. throw ; } } private void TrackException ( Exception ex ) { // For simplicity in this example, open the circuit breaker on the first exception. // In reality this would be more complex. A certain type of exception, such as one // that indicates a service is offline, might trip the circuit breaker immediately. // Alternatively it might count exceptions locally or across multiple instances and // use this value over time, or the exception/success ratio based on the exception // types, to open the circuit breaker. this . stateStore . Trip ( ex ); } } To use a CircuitBreaker object to protect an operation, an application creates an instance of the CircuitBreaker class and invokes the ExecuteAction method, specifying the operation to be performed as the parameter. The application should be prepared to catch the CircuitBreakerOpenException exception if the operation fails because the circuit breaker is open. var breaker = new CircuitBreaker (); try { breaker . ExecuteAction (() => { // Operation protected by the circuit breaker. ... }); } catch ( CircuitBreakerOpenException ex ) { // Perform some different action when the breaker is open. // Last exception details are in the inner exception. ... } catch ( Exception ex ) { ... }","title":"Circuit Breaker"},{"location":"notes/practices/Code%20review/","text":"Code review What to look for in a code review Design Do the interactions of various code changes make sense? Does it integrate well with the rest of the system? For example, a piece of code can be extracted to allow it to be used by other developers? Does the changes break certain design principles such as SOLID? Functionality As a code reviewer, always think of edge cases, look for concurrency issues, try to think like a end user, and making sure there is no bugs just by reading the code. Validate UI changes. See if there is any sort of parallel programming going on can cause deadlocks or race conditions. Complexity Is the changes too hard to understand for code reviewer? Avoid over-engineering, where developers make changes more generic than it needs to be. Functionalities that not presently needed by the system. Encourage developers they know that needs to be solved now rather than the problem developers speculate might need to be solved in the future. Test Ask for unit, integration, end-to-end tests for the changes. Tests should be added unless it is emergency problem. Make sure the tests are sensible and useful. Remember tests are also part of code has to be maintained, avoid complexity in tests. Naming Good naming for everything? A good name is long enough to fully communicate what it is or does. Comments Are comments clear in understandable english? Comments are useful to explain why some code exists, and should not explain what the code is doing. Exceptions include regular expressions and algorithms. Comments are different to documentation. Style Make sure certain style is followed. Style changes (fix formatting)0 should not mix with other changes. Documentation If code change how users build, test, interact or release code, check if appropriate documentation, including READMEs also need updating. Context It is helpful to look at the changes in a broad context, not just the changed lines. Think about the changes in the context of the system as a whole. Does it improve the code health of the whole system or does it make it more complex, less tested etc.? Good things If you see something nice in the changes, especially they addressed one of your comments in a great way, tell the developer. Code reviews mostly focus on bad things, but they should offer encouragements and appreciation for good practices. Summary The code is well-designed. The functionality is good for the users of the code. Any UI changes are sensible and look good. Any parallel programming is done safely. The code isn\u2019t more complex than it needs to be. The developer isn\u2019t implementing things they might need in the future but don\u2019t know they need now. Code has appropriate unit tests. Tests are well-designed. The developer used clear names for everything. Comments are clear and useful, and mostly explain why instead of what. Code is appropriately documented (generally in g3doc). The code conforms to our style guides.","title":"Code review"},{"location":"notes/practices/Code%20review/#code-review","text":"What to look for in a code review","title":"Code review"},{"location":"notes/practices/Code%20review/#design","text":"Do the interactions of various code changes make sense? Does it integrate well with the rest of the system? For example, a piece of code can be extracted to allow it to be used by other developers? Does the changes break certain design principles such as SOLID?","title":"Design"},{"location":"notes/practices/Code%20review/#functionality","text":"As a code reviewer, always think of edge cases, look for concurrency issues, try to think like a end user, and making sure there is no bugs just by reading the code. Validate UI changes. See if there is any sort of parallel programming going on can cause deadlocks or race conditions.","title":"Functionality"},{"location":"notes/practices/Code%20review/#complexity","text":"Is the changes too hard to understand for code reviewer? Avoid over-engineering, where developers make changes more generic than it needs to be. Functionalities that not presently needed by the system. Encourage developers they know that needs to be solved now rather than the problem developers speculate might need to be solved in the future.","title":"Complexity"},{"location":"notes/practices/Code%20review/#test","text":"Ask for unit, integration, end-to-end tests for the changes. Tests should be added unless it is emergency problem. Make sure the tests are sensible and useful. Remember tests are also part of code has to be maintained, avoid complexity in tests.","title":"Test"},{"location":"notes/practices/Code%20review/#naming","text":"Good naming for everything? A good name is long enough to fully communicate what it is or does.","title":"Naming"},{"location":"notes/practices/Code%20review/#comments","text":"Are comments clear in understandable english? Comments are useful to explain why some code exists, and should not explain what the code is doing. Exceptions include regular expressions and algorithms. Comments are different to documentation.","title":"Comments"},{"location":"notes/practices/Code%20review/#style","text":"Make sure certain style is followed. Style changes (fix formatting)0 should not mix with other changes.","title":"Style"},{"location":"notes/practices/Code%20review/#documentation","text":"If code change how users build, test, interact or release code, check if appropriate documentation, including READMEs also need updating.","title":"Documentation"},{"location":"notes/practices/Code%20review/#context","text":"It is helpful to look at the changes in a broad context, not just the changed lines. Think about the changes in the context of the system as a whole. Does it improve the code health of the whole system or does it make it more complex, less tested etc.?","title":"Context"},{"location":"notes/practices/Code%20review/#good-things","text":"If you see something nice in the changes, especially they addressed one of your comments in a great way, tell the developer. Code reviews mostly focus on bad things, but they should offer encouragements and appreciation for good practices.","title":"Good things"},{"location":"notes/practices/Code%20review/#summary","text":"The code is well-designed. The functionality is good for the users of the code. Any UI changes are sensible and look good. Any parallel programming is done safely. The code isn\u2019t more complex than it needs to be. The developer isn\u2019t implementing things they might need in the future but don\u2019t know they need now. Code has appropriate unit tests. Tests are well-designed. The developer used clear names for everything. Comments are clear and useful, and mostly explain why instead of what. Code is appropriately documented (generally in g3doc). The code conforms to our style guides.","title":"Summary"},{"location":"notes/react/Quick%20notes/","text":"React quick notes useCallback with no dependencies The following function is wrapped in useCallback , but it doesn't have any dependencies. const updateChildren = useCallback ( ( id : string , item : ITreeItem , children : ITreeItem []) => { if ( item . id === id ) { item . children . clear (); children . forEach (( child ) => item . children . set ( child . id , child )); return ; } for ( const [, value ] of item . children ) { updateChildren ( id , value , children ); } }, [] ); Purpose of useCallback does not depend on if you have dependencies or not. It's to ensure referential integrity. To get better performance. Every time your component re-render, a new instance of the function is created, useCallback is just an addition which assigns the reference to another variable. When you wrap a function with useCallback it actually memorizing(remember the last execution result) and the next time you call this function it will return the last result if none of the values in the dependent array is changed. Nested setTimeout vs setInterval Nested setTimeout allows to set the delay between the executions more precisely than setInterval . let i = 1 ; setInterval ( function () { func ( i ++ ); }, 100 ); let i = 1 ; setTimeout ( function run () { func ( i ++ ); setTimeout ( run , 100 ); }, 100 ); For setInterval the internal scheduler will run func(i++) every 100ms: The real delay between func calls for setInterval is less than in the code. That\u2019s normal, because the time taken by func 's execution consumes a part of the interval. It is possible that func 's execution turns out to be longer than we expected and takes more than 100ms. In this case the engine waits for func to complete, then checks the scheduler and if the time is up, runs it again immediately. In the edge case, if the function always executes longer than delay ms, then the calls will happen without a pause at all. And here is the picture for the nested setTimeout: The nested setTimeout guarantees the fixed delay (here 100ms). That\u2019s because a new call is planned at the end of the previous one. Cannot assign to 'current' because it i a read-only property When using React's useRef hook, if it is initialised with null default value, then tsserver will complain about the error. The reason is described in this comment . It's intentionally left readonly to ensure correct usage, even if it's not frozen. Refs initialized with null without specifically indicating you want to be able to assign null to it are interpreted as refs you want to be managed by React , i.e. React \"owns\" the current and you're just viewing it. If you want a mutable ref object that starts with a null value, make sure to also give | null to the generic argument. That will make it mutable, because you \"own\" it and not React. Difference between e.currentTarget and e.target e.currentTarget is the element the event is attached to. e.target is the element that triggers the event. Difference between e.preventDefault and e.stopPropagation e.preventDefault will stop the browser handling the event. e.stopPropagation will stop event bubbling up to the parent elements.","title":"React quick notes"},{"location":"notes/react/Quick%20notes/#react-quick-notes","text":"","title":"React quick notes"},{"location":"notes/react/Quick%20notes/#usecallback-with-no-dependencies","text":"The following function is wrapped in useCallback , but it doesn't have any dependencies. const updateChildren = useCallback ( ( id : string , item : ITreeItem , children : ITreeItem []) => { if ( item . id === id ) { item . children . clear (); children . forEach (( child ) => item . children . set ( child . id , child )); return ; } for ( const [, value ] of item . children ) { updateChildren ( id , value , children ); } }, [] ); Purpose of useCallback does not depend on if you have dependencies or not. It's to ensure referential integrity. To get better performance. Every time your component re-render, a new instance of the function is created, useCallback is just an addition which assigns the reference to another variable. When you wrap a function with useCallback it actually memorizing(remember the last execution result) and the next time you call this function it will return the last result if none of the values in the dependent array is changed.","title":"useCallback with no dependencies"},{"location":"notes/react/Quick%20notes/#nested-settimeout-vs-setinterval","text":"Nested setTimeout allows to set the delay between the executions more precisely than setInterval . let i = 1 ; setInterval ( function () { func ( i ++ ); }, 100 ); let i = 1 ; setTimeout ( function run () { func ( i ++ ); setTimeout ( run , 100 ); }, 100 ); For setInterval the internal scheduler will run func(i++) every 100ms: The real delay between func calls for setInterval is less than in the code. That\u2019s normal, because the time taken by func 's execution consumes a part of the interval. It is possible that func 's execution turns out to be longer than we expected and takes more than 100ms. In this case the engine waits for func to complete, then checks the scheduler and if the time is up, runs it again immediately. In the edge case, if the function always executes longer than delay ms, then the calls will happen without a pause at all. And here is the picture for the nested setTimeout: The nested setTimeout guarantees the fixed delay (here 100ms). That\u2019s because a new call is planned at the end of the previous one.","title":"Nested setTimeout vs setInterval"},{"location":"notes/react/Quick%20notes/#cannot-assign-to-current-because-it-i-a-read-only-property","text":"When using React's useRef hook, if it is initialised with null default value, then tsserver will complain about the error. The reason is described in this comment . It's intentionally left readonly to ensure correct usage, even if it's not frozen. Refs initialized with null without specifically indicating you want to be able to assign null to it are interpreted as refs you want to be managed by React , i.e. React \"owns\" the current and you're just viewing it. If you want a mutable ref object that starts with a null value, make sure to also give | null to the generic argument. That will make it mutable, because you \"own\" it and not React.","title":"Cannot assign to 'current' because it i a read-only property"},{"location":"notes/react/Quick%20notes/#difference-between-ecurrenttarget-and-etarget","text":"e.currentTarget is the element the event is attached to. e.target is the element that triggers the event.","title":"Difference between e.currentTarget and e.target"},{"location":"notes/react/Quick%20notes/#difference-between-epreventdefault-and-estoppropagation","text":"e.preventDefault will stop the browser handling the event. e.stopPropagation will stop event bubbling up to the parent elements.","title":"Difference between e.preventDefault and e.stopPropagation"},{"location":"notes/react/React%20testing%20library/","text":"React testing library Always use Queries when query for elements in DOM I wrote a test using react-testing-library and msw to test expand and collapse a tree. It fires 2 click events, the first will expand the tree root node and the second will collapse it. it ( 'toggles node expansion states when clicking' , async () => { renderFolderTree (); setupFolderData (); const rootFolder = await screen . findByText ( 'root' ); expect ( screen . queryByText ( 'child1' )). not . toBeInTheDocument (); expect ( screen . queryByText ( 'child2' )). not . toBeInTheDocument (); setupFolderData (); fireEvent . click ( rootFolder ); // Wait for async data fetching. await waitFor (() => screen . getByText ( 'child1' )); expect ( screen . getByText ( 'child1' )). toBeInTheDocument (); expect ( screen . getByText ( 'child2' )). toBeInTheDocument (); fireEvent . click ( rootFolder ); expect ( screen . getByText ( 'child1' )). not . toBeInTheDocument (); expect ( screen . getByText ( 'child2' )). not . toBeInTheDocument (); }); The test failed because child1 and child2 are still in DOM after the second click. It turns out to be that the event for seconds click on rootFolder element never triggered the actual clicked event handler in the tree component. This is fixed by always using Queries when querying elements in the DOM. fireEvent . click ( screen . getByText ( 'root' )); The lesson learnt is do not keep local reference of queried element, instead alway use Queries, the reference of already queried elements may go out of scope and fireEvent will not trigger the event handlers.","title":"React testing library"},{"location":"notes/react/React%20testing%20library/#react-testing-library","text":"","title":"React testing library"},{"location":"notes/react/React%20testing%20library/#always-use-queries-when-query-for-elements-in-dom","text":"I wrote a test using react-testing-library and msw to test expand and collapse a tree. It fires 2 click events, the first will expand the tree root node and the second will collapse it. it ( 'toggles node expansion states when clicking' , async () => { renderFolderTree (); setupFolderData (); const rootFolder = await screen . findByText ( 'root' ); expect ( screen . queryByText ( 'child1' )). not . toBeInTheDocument (); expect ( screen . queryByText ( 'child2' )). not . toBeInTheDocument (); setupFolderData (); fireEvent . click ( rootFolder ); // Wait for async data fetching. await waitFor (() => screen . getByText ( 'child1' )); expect ( screen . getByText ( 'child1' )). toBeInTheDocument (); expect ( screen . getByText ( 'child2' )). toBeInTheDocument (); fireEvent . click ( rootFolder ); expect ( screen . getByText ( 'child1' )). not . toBeInTheDocument (); expect ( screen . getByText ( 'child2' )). not . toBeInTheDocument (); }); The test failed because child1 and child2 are still in DOM after the second click. It turns out to be that the event for seconds click on rootFolder element never triggered the actual clicked event handler in the tree component. This is fixed by always using Queries when querying elements in the DOM. fireEvent . click ( screen . getByText ( 'root' )); The lesson learnt is do not keep local reference of queried element, instead alway use Queries, the reference of already queried elements may go out of scope and fireEvent will not trigger the event handlers.","title":"Always use Queries when query for elements in DOM"},{"location":"notes/security/JWT/","text":"JWT Why do we need the JSON Web Token (JWT) in the modern web? The Problem The HTTP protocol is stateless, it means each request has no idea about the previous request, so we need to reauthenticate for each request. The traditional way of solving the problem is to use server side sessions. Once the server checks username and password it will create a session id and save it in memory, the client will then use session id to communicate. The solution isn\u2019t scale, as the server needs to maintain a huge amount of sessions and multiple servers need to share all the session information. How JWT solves the problem JWT Content JWT is self-contained, meaning all the information required by the server to approve or reject a request to an API. A JWT has header, payload and signature. The header section contains information about the token itself. { \"kid\" : \"ywdoAL4WL...rV4InvRo=\" , \"alg\" : \"RS256\" } The above header explains the algorithm (alg) used to sign the token and the key (kid) we need to validate it. The payload section contains information about the client (claims etc.). { [ ... ] \"iss\" : \"https://cognito-idp.eu-west-1.amazonaws.com/XXX\" , \"name\" : \"Mariano Calandra\" , \"admin\" : false } The iss property is a registered claim, it represents the identity provider that issues the token, in this case AWS cognito. We can add further claims based on our needs, such as \u201cadmin\u201d. The third section is signature, it is a hash that computed by the following step: * Join with a dot the encoded header and the encoded payload; * Hash the result using \u201calg\u201d (RS256) and a private key; * Encode the result as Base64URL; data = base64UrlEncode(header) + \".\" + base64UrlEncode(payload); hash = RS256(data, private_key); signature = base64UrlEncode(hash); Note that Base64URL is not an encryption algorithm, the payload is NOT encrypted, so DO NOT put sensitive information in the payload. JWT Validation Since JWT is self-contained, we have all the information needed to validate the token. We now need to know how to get the public key to perform the validation. Note that in asymmetric encryption, the public key is used to encrypt a message, private key is used to decrypt it. In the signing algorithm it is opposite. In a JWT data is signed using a private key and the public key is used to verify the data. The iss property in a header is the endpoint of the issuer. Following the endpoint we get: { \"keys\" : [ { \"alg\" : \"RS256\" , \"e\" : \"AQAB\" , \"kid\" : \"ywdoAL4WL...rV4InvRo=\" , \"kty\" : \"RSA\" , \"n\" : \"m7uImGR -TRUNCATED AhaabmiCq5WMQ\" , \"use\" : \"sig\" }, { ... } ] } In the keys array, search for the element with the same \u201ckid\u201d in a JWT\u2019s header, the \u201ce\u201d and \u201cn\u201d are the public exponent and modulus that compute the public key. At the first request, a client needs to contact the authentication server, sending username and password to it. If credentials are valid, a JWT token will be returned to the client that will use it to request an API.","title":"JWT"},{"location":"notes/security/JWT/#jwt","text":"Why do we need the JSON Web Token (JWT) in the modern web?","title":"JWT"},{"location":"notes/security/JWT/#the-problem","text":"The HTTP protocol is stateless, it means each request has no idea about the previous request, so we need to reauthenticate for each request. The traditional way of solving the problem is to use server side sessions. Once the server checks username and password it will create a session id and save it in memory, the client will then use session id to communicate. The solution isn\u2019t scale, as the server needs to maintain a huge amount of sessions and multiple servers need to share all the session information.","title":"The Problem"},{"location":"notes/security/JWT/#how-jwt-solves-the-problem","text":"","title":"How JWT solves the problem"},{"location":"notes/security/JWT/#jwt-content","text":"JWT is self-contained, meaning all the information required by the server to approve or reject a request to an API. A JWT has header, payload and signature. The header section contains information about the token itself. { \"kid\" : \"ywdoAL4WL...rV4InvRo=\" , \"alg\" : \"RS256\" } The above header explains the algorithm (alg) used to sign the token and the key (kid) we need to validate it. The payload section contains information about the client (claims etc.). { [ ... ] \"iss\" : \"https://cognito-idp.eu-west-1.amazonaws.com/XXX\" , \"name\" : \"Mariano Calandra\" , \"admin\" : false } The iss property is a registered claim, it represents the identity provider that issues the token, in this case AWS cognito. We can add further claims based on our needs, such as \u201cadmin\u201d. The third section is signature, it is a hash that computed by the following step: * Join with a dot the encoded header and the encoded payload; * Hash the result using \u201calg\u201d (RS256) and a private key; * Encode the result as Base64URL; data = base64UrlEncode(header) + \".\" + base64UrlEncode(payload); hash = RS256(data, private_key); signature = base64UrlEncode(hash); Note that Base64URL is not an encryption algorithm, the payload is NOT encrypted, so DO NOT put sensitive information in the payload.","title":"JWT Content"},{"location":"notes/security/JWT/#jwt-validation","text":"Since JWT is self-contained, we have all the information needed to validate the token. We now need to know how to get the public key to perform the validation. Note that in asymmetric encryption, the public key is used to encrypt a message, private key is used to decrypt it. In the signing algorithm it is opposite. In a JWT data is signed using a private key and the public key is used to verify the data. The iss property in a header is the endpoint of the issuer. Following the endpoint we get: { \"keys\" : [ { \"alg\" : \"RS256\" , \"e\" : \"AQAB\" , \"kid\" : \"ywdoAL4WL...rV4InvRo=\" , \"kty\" : \"RSA\" , \"n\" : \"m7uImGR -TRUNCATED AhaabmiCq5WMQ\" , \"use\" : \"sig\" }, { ... } ] } In the keys array, search for the element with the same \u201ckid\u201d in a JWT\u2019s header, the \u201ce\u201d and \u201cn\u201d are the public exponent and modulus that compute the public key. At the first request, a client needs to contact the authentication server, sending username and password to it. If credentials are valid, a JWT token will be returned to the client that will use it to request an API.","title":"JWT Validation"},{"location":"notes/til/til/","text":"Today I Learned 05/03/2022 - Today I was doing a leetcode question , and I couldn't get one of the tests to pass, and eventually I figured out that JavaScript's array.sort() method by default sorts the elements alphabetically. [-68, -96, -12, -40, 16].sort() will give you [ -12, -40, -68, -96, 16 ] . Today I learned you have to pass in a comparer function if you want to sort numerically. arr.sort((a, b) => a - b); 16/03/2022 - I alway forget how to property use the spread operator ... in Java/TypeScript. The following is a good example of how to replace an specific field of an array of objects. The ... will spread the objects in the array and replace with name property with the specified value. // \u2705 Updating properties in multiple objects const arr1 = [ { id : 1 , name : \"Alice\" }, { id : 1 , name : \"Bob\" }, { id : 3 , name : \"Charlie\" }, ]; // Returns a new array // [ // { id: 1, name: \"Alfred\" }, // { id: 1, name: \"Bob\" }, // { id: 3, name: \"Charlie\" }, // ] const newArr = arr1 . map (( obj ) => { if ( obj . id === 1 ) { return { ... obj , name : \"Alfred\" }; } return obj ; });","title":"Today I Learned"},{"location":"notes/til/til/#today-i-learned","text":"05/03/2022 - Today I was doing a leetcode question , and I couldn't get one of the tests to pass, and eventually I figured out that JavaScript's array.sort() method by default sorts the elements alphabetically. [-68, -96, -12, -40, 16].sort() will give you [ -12, -40, -68, -96, 16 ] . Today I learned you have to pass in a comparer function if you want to sort numerically. arr.sort((a, b) => a - b); 16/03/2022 - I alway forget how to property use the spread operator ... in Java/TypeScript. The following is a good example of how to replace an specific field of an array of objects. The ... will spread the objects in the array and replace with name property with the specified value. // \u2705 Updating properties in multiple objects const arr1 = [ { id : 1 , name : \"Alice\" }, { id : 1 , name : \"Bob\" }, { id : 3 , name : \"Charlie\" }, ]; // Returns a new array // [ // { id: 1, name: \"Alfred\" }, // { id: 1, name: \"Bob\" }, // { id: 3, name: \"Charlie\" }, // ] const newArr = arr1 . map (( obj ) => { if ( obj . id === 1 ) { return { ... obj , name : \"Alfred\" }; } return obj ; });","title":"Today I Learned"}]}